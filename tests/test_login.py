#!/usr/bin/env python3
"""
ERPç™»å½•åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.crawlers.auth import ERPAuthCrawler
from app.config.settings import settings
from app.utils.logger import setup_logger


async def test_login():
    """æµ‹è¯•ERPç™»å½•åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ERPç™»å½•åŠŸèƒ½...")
    print(f"ğŸ“ ERPç³»ç»Ÿ: {settings.erp_base_url}")
    print(f"ğŸ‘¤ ç”¨æˆ·å: {settings.erp_username}")
    print(f"ğŸ”§ æµè§ˆå™¨æ¨¡å¼: {'æœ‰å¤´æ¨¡å¼' if not settings.browser_headless else 'æ— å¤´æ¨¡å¼'}")
    print("-" * 50)

    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    setup_logger()

    # åˆ›å»ºç™»å½•çˆ¬è™«å®ä¾‹
    crawler = ERPAuthCrawler()

    try:
        print("ğŸ”„ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")

        # æ‰§è¡Œç™»å½•æµç¨‹
        async with crawler.browser_session():
            print("ğŸ“– æµè§ˆå™¨å·²å¯åŠ¨ï¼Œå¼€å§‹ç™»å½•æµç¨‹...")

            # æ£€æŸ¥å½“å‰æ˜¯å¦å·²ç»ç™»å½•
            if await crawler.check_login_status():
                print("âœ… å·²ç»å¤„äºç™»å½•çŠ¶æ€")
                return True

            # å…ˆå¯¼èˆªåˆ°ç™»å½•é¡µé¢
            print("ğŸŒ å¯¼èˆªåˆ°ç™»å½•é¡µé¢...")
            await crawler.navigate_to(settings.erp_login_page)

            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ŒåŒ…æ‹¬å¯èƒ½çš„JavaScriptæ¸²æŸ“
            print("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
            await crawler.page.wait_for_load_state("networkidle")
            await asyncio.sleep(5)  # é¢å¤–ç­‰å¾…5ç§’è®©JavaScriptæœ‰æ—¶é—´æ¸²æŸ“

            # è°ƒè¯•ï¼šè·å–é¡µé¢HTMLç»“æ„å’Œæ ‡é¢˜
            page_title = await crawler.page.title()
            current_url = crawler.page.url
            print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
            print(f"ğŸ”— å½“å‰URL: {current_url}")

            # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢å†…å®¹
            page_content = await crawler.page.content()
            print(f"ğŸ“ é¡µé¢HTMLé•¿åº¦: {len(page_content)} å­—ç¬¦")

            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯é¡µé¢æˆ–è·³è½¬
            if "404" in page_content or "é¡µé¢ä¸å­˜åœ¨" in page_content or "æ‰¾ä¸åˆ°" in page_content:
                print("âš ï¸ é¡µé¢å¯èƒ½ä¸å­˜åœ¨æˆ–å‡ºç°404é”™è¯¯")

            # è°ƒè¯•ï¼šæŸ¥çœ‹é¡µé¢ä¸­æ‰€æœ‰çš„inputå…ƒç´ 
            inputs = await crawler.page.query_selector_all("input")
            print(f"ğŸ” æ‰¾åˆ° {len(inputs)} ä¸ªinputå…ƒç´ :")
            for i, input_elem in enumerate(inputs):
                try:
                    placeholder = await input_elem.get_attribute("placeholder")
                    input_type = await input_elem.get_attribute("type")
                    name = await input_elem.get_attribute("name")
                    id_attr = await input_elem.get_attribute("id")
                    class_name = await input_elem.get_attribute("class")
                    print(f"  {i+1}. Type: {input_type}, Name: {name}, ID: {id_attr}, Class: {class_name}, Placeholder: {placeholder}")
                except:
                    print(f"  {i+1}. æ— æ³•è·å–å±æ€§")

            # è°ƒè¯•ï¼šæŸ¥çœ‹é¡µé¢ä¸­æ‰€æœ‰çš„buttonå…ƒç´ 
            buttons = await crawler.page.query_selector_all("button")
            print(f"ğŸ” æ‰¾åˆ° {len(buttons)} ä¸ªbuttonå…ƒç´ :")
            for i, button in enumerate(buttons):
                try:
                    text = await button.text_content()
                    class_name = await button.get_attribute("class")
                    print(f"  {i+1}. Text: {text}, Class: {class_name}")
                except:
                    print(f"  {i+1}. æ— æ³•è·å–å±æ€§")

            # è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦æœ‰iframe
            iframes = await crawler.page.query_selector_all("iframe")
            print(f"ğŸ” æ‰¾åˆ° {len(iframes)} ä¸ªiframeå…ƒç´ ")
            for i, iframe in enumerate(iframes):
                try:
                    src = await iframe.get_attribute("src")
                    print(f"  {i+1}. iframe src: {src}")
                except:
                    print(f"  {i+1}. æ— æ³•è·å–iframeå±æ€§")

            # è°ƒè¯•ï¼šä¿å­˜é¡µé¢HTMLåˆ°æ–‡ä»¶
            html_file = "temp/debug_page.html"
            Path(html_file).write_text(page_content, encoding='utf-8')
            print(f"ğŸ’¾ é¡µé¢HTMLå·²ä¿å­˜åˆ°: {html_file}")

            # æˆªå›¾ä¿å­˜å½“å‰é¡µé¢çŠ¶æ€
            debug_screenshot = await crawler.take_screenshot("debug_login_page.png")
            print(f"ğŸ“¸ è°ƒè¯•æˆªå›¾å·²ä¿å­˜: {debug_screenshot}")

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨å•å…ƒç´ ï¼Œå†ç­‰å¾…ä¸€ä¸‹
            if len(inputs) == 0 and len(buttons) == 0:
                print("â³ æœªæ‰¾åˆ°è¡¨å•å…ƒç´ ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´...")
                await asyncio.sleep(10)

                # å†æ¬¡æ£€æŸ¥
                inputs = await crawler.page.query_selector_all("input")
                buttons = await crawler.page.query_selector_all("button")
                print(f"ğŸ”„ å†æ¬¡æ£€æŸ¥ - æ‰¾åˆ° {len(inputs)} ä¸ªinputå…ƒç´ , {len(buttons)} ä¸ªbuttonå…ƒç´ ")

            print("ğŸ” å¼€å§‹æ‰§è¡Œç™»å½•æ“ä½œ...")
            login_success = await crawler.login()

            if login_success:
                print("ğŸ‰ ç™»å½•æˆåŠŸï¼")

                # å†æ¬¡æ£€æŸ¥ç™»å½•çŠ¶æ€
                if await crawler.check_login_status():
                    print("âœ… ç™»å½•çŠ¶æ€éªŒè¯é€šè¿‡")

                    # è·å–å½“å‰é¡µé¢ä¿¡æ¯
                    current_url = crawler.page.url
                    page_title = await crawler.page.title()
                    print(f"ğŸ“„ å½“å‰é¡µé¢: {page_title}")
                    print(f"ğŸ”— å½“å‰URL: {current_url}")

                    # æˆªå›¾ä¿å­˜ç™»å½•æˆåŠŸçŠ¶æ€
                    screenshot_path = await crawler.take_screenshot("login_success.png")
                    print(f"ğŸ“¸ ç™»å½•æˆåŠŸæˆªå›¾å·²ä¿å­˜: {screenshot_path}")

                    await asyncio.sleep(10)
                    return True
                else:
                    print("âŒ ç™»å½•çŠ¶æ€éªŒè¯å¤±è´¥")
                    return False
            else:
                print("âŒ ç™»å½•å¤±è´¥")
                return False

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")

        # å°è¯•æˆªå›¾ä¿å­˜é”™è¯¯çŠ¶æ€
        try:
            if crawler.page:
                screenshot_path = await crawler.take_screenshot("login_error.png")
                print(f"ğŸ“¸ é”™è¯¯çŠ¶æ€æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
        except:
            pass

        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª ERPç™»å½•åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    success = await test_login()

    print("-" * 50)
    if success:
        print("ğŸ¯ æµ‹è¯•ç»“æœ: æˆåŠŸ âœ…")
        print("ğŸš€ ERPç™»å½•åŠŸèƒ½å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥å¼€å‘")
    else:
        print("ğŸ¯ æµ‹è¯•ç»“æœ: å¤±è´¥ âŒ")
        print("ğŸ”§ è¯·æ£€æŸ¥ç™»å½•å‚æ•°æˆ–ERPç³»ç»Ÿé¡µé¢ç»“æ„")

    print("=" * 60)

    # ä¿æŒæµè§ˆå™¨çª—å£æ‰“å¼€ä¸€æ®µæ—¶é—´ä¾›è§‚å¯Ÿ
    if success:
        print("â° æµè§ˆå™¨å°†åœ¨10ç§’åè‡ªåŠ¨å…³é—­...")
        await asyncio.sleep(10)


if __name__ == "__main__":
    asyncio.run(main())